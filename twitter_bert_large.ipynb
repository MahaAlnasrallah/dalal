{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29dcaf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path\n",
    "from arabert.arabert.preprocess import ArabertPreprocessor\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, precision_score , recall_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e19abbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, BertTokenizer,AutoModel\n",
    "from transformers.data.processors import SingleSentenceClassificationProcessor\n",
    "from transformers import Trainer , TrainingArguments\n",
    "from transformers.trainer_utils import EvaluationStrategy\n",
    "from transformers.data.processors.utils import InputFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91e4f800",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.utils import resample\n",
    "import logging\n",
    "import torch\n",
    "import csv\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "194d191e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1f477fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "    !nvidia-smi\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2798c627",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        name,\n",
    "        train,\n",
    "        test,\n",
    "        label_list,\n",
    "    ):\n",
    "        self.name = name\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.label_list = label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff78341f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"GAZTTweets1.xlsx\"\n",
    "df = pd.read_excel(path,header=0)\n",
    "model_name = \"bert-base-arabertv02\"\n",
    "arabert_prep = ArabertPreprocessor(model_name=model_name, keep_emojis=False,replace_urls_emails_mentions=True)\n",
    "\n",
    "DATA_COLUMN = 'text'\n",
    "LABEL_COLUMN = 'label'\n",
    "Q_COLUMN= 'question'\n",
    "df = df[['text', 'sentiment']]\n",
    "df.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
    "\n",
    "label_map = {\n",
    "    'negative' : 0,\n",
    "    'neutral': 2,\n",
    "    'positive' : 1\n",
    "\n",
    "}\n",
    "df['orginal'] = df[DATA_COLUMN]\n",
    "df[DATA_COLUMN] = df[DATA_COLUMN].apply(arabert_prep.preprocess)\n",
    "df[LABEL_COLUMN] = df[LABEL_COLUMN].apply(lambda x: label_map[x])\n",
    "\n",
    "\n",
    "train_AJGT, test_AJGT = train_test_split(df, test_size=0.2,random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "train_df = pd.DataFrame({\n",
    "    'id':range(len(train_AJGT)),\n",
    "    'label':train_AJGT[\"label\"],\n",
    "    'alpha':['a']*train_AJGT.shape[0],\n",
    "    'text': train_AJGT[\"text\"].replace(r'\\n', ' ', regex=True),\n",
    "    'orginal': train_AJGT[\"orginal\"].replace(r'\\n', ' ', regex=True)\n",
    "})\n",
    "\n",
    "dev_df = pd.DataFrame({\n",
    "    'id':range(len(test_AJGT)),\n",
    "    'label':test_AJGT[\"label\"],\n",
    "    'alpha':['a']*test_AJGT.shape[0],\n",
    "    'text': test_AJGT[\"text\"].replace(r'\\n', ' ', regex=True),\n",
    "    'orginal': test_AJGT[\"orginal\"].replace(r'\\n', ' ', regex=True)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dae3d5b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(929, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f97f08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.WARNING)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3e4c4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, text, target, model_name, max_len, label_map):\n",
    "      super(BERTDataset).__init__()\n",
    "      self.text = text\n",
    "      self.target = target\n",
    "      self.tokenizer_name = model_name\n",
    "      self.tokenizer = BertTokenizer.from_pretrained(model_name,from_pt=True)\n",
    "      self.max_len = max_len\n",
    "      self.label_map = label_map\n",
    "      \n",
    "\n",
    "    def __len__(self):\n",
    "      return len(self.text)\n",
    "\n",
    "    def __getitem__(self,item):\n",
    "      text = str(self.text[item])\n",
    "      text = \" \".join(text.split())\n",
    "\n",
    "\n",
    "        \n",
    "      input_ids = self.tokenizer.encode(\n",
    "          text,\n",
    "          add_special_tokens=True,\n",
    "          max_length=self.max_len,\n",
    "          truncation='longest_first'\n",
    "      )     \n",
    "    \n",
    "      attention_mask = [1] * len(input_ids)\n",
    "\n",
    "      # Zero-pad up to the sequence length.\n",
    "      padding_length = self.max_len - len(input_ids)\n",
    "      input_ids = input_ids + ([self.tokenizer.pad_token_id] * padding_length)\n",
    "      attention_mask = attention_mask + ([0] * padding_length)    \n",
    "      \n",
    "      return InputFeatures(input_ids=input_ids, attention_mask=attention_mask, label=self.label_map[self.target[item]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b35ad838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ولن نبالغ إذا قلنا إن هاتف أو كمبيوتر المكتب في زمننا هذا ضروري'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name=\"bert-base-arabertv02\"\n",
    "arabert_prep = ArabertPreprocessor(model_name=model_name)\n",
    "\n",
    "text = \"ولن نبالغ إذا قلنا إن هاتف أو كمبيوتر المكتب في زمننا هذا ضروري\"\n",
    "arabert_prep.preprocess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52d290db",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = { v:index for index, v in enumerate(df[LABEL_COLUMN].unique()) }\n",
    "max_len=64\n",
    "#print(label_map)\n",
    "model_name='bert-large-arabertv02-twitter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c2e6515",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BERTDataset(train_df['text'].to_list(),train_df['label'].to_list(),model_name,max_len,label_map)\n",
    "test_dataset = BERTDataset(dev_df['text'].to_list(),dev_df['label'].to_list(),model_name,max_len,label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f341dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "save_obj(label_map,'label_map')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eae755e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\"bert-large-arabertv02-twitter\", return_dict=True, num_labels=len(label_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d1dc78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p): #p should be of type EvalPrediction\n",
    "  preds = np.argmax(p.predictions, axis=1)\n",
    "  assert len(preds) == len(p.label_ids)\n",
    "  #print(classification_report(p.label_ids,preds))\n",
    "  #print(confusion_matrix(p.label_ids,preds))\n",
    "\n",
    "  macro_f1_pos_neg = f1_score(p.label_ids,preds,average='macro',labels=[0,1])\n",
    "  macro_f1 = f1_score(p.label_ids,preds,average='macro')\n",
    "  macro_precision = precision_score(p.label_ids,preds,average='macro')\n",
    "  macro_recall = recall_score(p.label_ids,preds,average='macro')\n",
    "  acc = accuracy_score(p.label_ids,preds)\n",
    "  return {\n",
    "      'macro_f1' : macro_f1,\n",
    "      'macro_f1_pos_neg' : macro_f1_pos_neg,  \n",
    "      'macro_precision': macro_precision,\n",
    "      'macro_recall': macro_recall,\n",
    "      'accuracy': acc\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5d5c58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "55\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\"./train\")\n",
    "training_args.evaluate_during_training = True\n",
    "training_args.adam_epsilon = 1e-8\n",
    "training_args.learning_rate = 5e-5\n",
    "training_args.fp16 = True\n",
    "training_args.per_device_train_batch_size = 32\n",
    "training_args.per_device_eval_batch_size = 32\n",
    "training_args.gradient_accumulation_steps = 2\n",
    "training_args.num_train_epochs= 5\n",
    "\n",
    "\n",
    "steps_per_epoch = (len(train_df)// (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps))\n",
    "total_steps = steps_per_epoch * training_args.num_train_epochs\n",
    "print(steps_per_epoch)\n",
    "print(total_steps)\n",
    "#Warmup_ratio\n",
    "warmup_ratio = 0.1\n",
    "training_args.warmup_steps = total_steps*warmup_ratio # or you can set the warmup steps directly \n",
    "\n",
    "training_args.evaluation_strategy = EvaluationStrategy.EPOCH\n",
    "training_args.logging_steps = 200\n",
    "training_args.save_steps = 100000 #don't want to save any model, there is probably a better way to do this :)\n",
    "training_args.seed = 42\n",
    "training_args.disable_tqdm = False\n",
    "training_args.lr_scheduler_type = 'cosine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "643b584e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb4f31ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-arabertv02-twitter were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-arabertv02-twitter and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('bert-large-arabertv02-twitter', return_dict=True, num_labels=len(label_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2be98b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-arabertv02-twitter were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-arabertv02-twitter and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\96654\\anaconda3\\lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model = model_init(),\n",
    "    args = training_args,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b81e8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\96654\\anaconda3\\lib\\site-packages\\torch\\autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 1:51:44, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Macro F1 Pos Neg</th>\n",
       "      <th>Macro Precision</th>\n",
       "      <th>Macro Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.443863</td>\n",
       "      <td>0.845954</td>\n",
       "      <td>0.827754</td>\n",
       "      <td>0.872162</td>\n",
       "      <td>0.828924</td>\n",
       "      <td>0.844086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.245351</td>\n",
       "      <td>0.893246</td>\n",
       "      <td>0.866896</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.887413</td>\n",
       "      <td>0.887097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.233931</td>\n",
       "      <td>0.913924</td>\n",
       "      <td>0.891434</td>\n",
       "      <td>0.926091</td>\n",
       "      <td>0.904417</td>\n",
       "      <td>0.908602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.218475</td>\n",
       "      <td>0.937508</td>\n",
       "      <td>0.926809</td>\n",
       "      <td>0.943910</td>\n",
       "      <td>0.932799</td>\n",
       "      <td>0.935484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.219821</td>\n",
       "      <td>0.928646</td>\n",
       "      <td>0.913517</td>\n",
       "      <td>0.936448</td>\n",
       "      <td>0.922046</td>\n",
       "      <td>0.924731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\96654\\anaconda3\\lib\\site-packages\\torch\\autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "C:\\Users\\96654\\anaconda3\\lib\\site-packages\\torch\\autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "C:\\Users\\96654\\anaconda3\\lib\\site-packages\\torch\\autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "C:\\Users\\96654\\anaconda3\\lib\\site-packages\\torch\\autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=60, training_loss=0.3490442276000977)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e569b7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'macro_f1': 0.9286461974492206,\n",
       " 'macro_f1_pos_neg': 0.9135172413793103,\n",
       " 'macro_precision': 0.9364478114478114,\n",
       " 'macro_recall': 0.9220463010489465,\n",
       " 'accuracy': 0.9247311827956989}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(r'C:\\Users\\96654\\twee3\\twitter_large')\n",
    "\n",
    "trainer.compute_metrics(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "15da3bdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 01:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.47%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.90      0.90        62\n",
      "           1       1.00      0.92      0.96        38\n",
      "           2       0.92      0.94      0.93        86\n",
      "\n",
      "    accuracy                           0.92       186\n",
      "   macro avg       0.94      0.92      0.93       186\n",
      "weighted avg       0.93      0.92      0.93       186\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result=trainer.predict(test_dataset)\n",
    "final_pred=np.argmax(result.predictions, axis=1)\n",
    "final_result=[list(label_map.keys())[list(label_map.values()).index(x)] for x in final_pred]\n",
    "true_label=dev_df['label'].to_list()\n",
    "true_text=dev_df['text'].to_list()\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(true_label, final_result)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "print(classification_report(true_label, final_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f937176d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentResults = pd.DataFrame(\n",
    "    {'text': true_text,\n",
    "     'sentiment': true_label,\n",
    "     'sentiment_BERT': final_result\n",
    "    })\n",
    "sentimentResults.to_excel('BERT_tweets_RESULTS5.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f03291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
